{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1hcd7qZDWjKaLmRbR34LzGO0y1Q_ITo0C","authorship_tag":"ABX9TyNzseH3nvEdv0kzTay4ejUH"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Package Installations"],"metadata":{"id":"3Y9cquxhf-6S"}},{"cell_type":"code","source":["# !pip install langgraph\n","# !pip install rank_bm25\n","# !pip install flair\n","# !pip install faiss-cpu"],"metadata":{"id":"NdNNWUslHkex","executionInfo":{"status":"ok","timestamp":1756197073837,"user_tz":-330,"elapsed":5,"user":{"displayName":"akshay chauhan","userId":"17758583881823287951"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["# import nltk\n","# nltk.download('punkt_tab')\n","# !pip install --upgrade google-genai"],"metadata":{"id":"VA1uqz-Vsh1W","executionInfo":{"status":"ok","timestamp":1756197079153,"user_tz":-330,"elapsed":9,"user":{"displayName":"akshay chauhan","userId":"17758583881823287951"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["# library imports"],"metadata":{"id":"It33exiao7g1"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"m2A_qO9Voy7q","executionInfo":{"status":"ok","timestamp":1756217101008,"user_tz":-330,"elapsed":41345,"user":{"displayName":"akshay chauhan","userId":"17758583881823287951"}}},"outputs":[],"source":["from collections import defaultdict\n","from datetime import datetime\n","from flair.data import Sentence\n","from flair.models import SequenceTagger\n","\n","from google import genai\n","from google.cloud import aiplatform\n","from google.genai import types\n","from google.genai.types import HarmBlockThreshold\n","from google.genai.types import HarmCategory\n","import google.auth\n","\n","import faiss\n","import numpy as np\n","import pandas as pd\n","import os\n","import pandas\n","import pickle\n","import json\n","\n","from langchain.output_parsers import PydanticOutputParser\n","from langchain.tools import tool\n","# from langchain_community.vectorstores import FAISS\n","from langchain_core.callbacks import CallbackManagerForLLMRun\n","from langchain_core.documents import Document\n","from langchain_core.embeddings import Embeddings\n","from langchain_core.language_models import BaseLLM\n","from langchain_core.outputs import Generation,LLMResult\n","from langchain.prompts import PromptTemplate\n","from langgraph.graph import END,StateGraph\n","\n","from nltk.tokenize import word_tokenize\n","from pydantic import BaseModel, Field\n","from rank_bm25 import BM25Okapi\n","from sentence_transformers import CrossEncoder\n","from sklearn.preprocessing import normalize\n","import re\n","\n","from typing import Any, Dict, List, Optional, Union\n","from typing_extensions import Optional, TypedDict\n","import vertexai\n","from vertexai.generative_models import FinishReason, GenerativeModel, Part, SafetySetting\n","from vertexai.language_models import TextEmbedding, TextEmbeddingModel\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","from time import time"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nSNkzWYz8dM_","executionInfo":{"status":"ok","timestamp":1756217192721,"user_tz":-330,"elapsed":1851,"user":{"displayName":"akshay chauhan","userId":"17758583881823287951"}},"outputId":"dbdd84bb-6c53-419d-ded4-0d0f96e74c2d"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["# Prompt import\n","import sys\n","sys.path.append('/content/drive/MyDrive/Colab Notebooks/Conversational BI')\n","from master_prompt import db1Main,db1Clarification,summarizerPrompt\n"],"metadata":{"id":"et365xCmC6Ef","executionInfo":{"status":"ok","timestamp":1756217197096,"user_tz":-330,"elapsed":8,"user":{"displayName":"akshay chauhan","userId":"17758583881823287951"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["# print(db1Main)"],"metadata":{"id":"FrQZ9LrpCvau","executionInfo":{"status":"ok","timestamp":1756217197580,"user_tz":-330,"elapsed":39,"user":{"displayName":"akshay chauhan","userId":"17758583881823287951"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["# authentication\n","from google.colab import auth\n","auth.authenticate_user()"],"metadata":{"id":"vOe_dS0sgZRw","executionInfo":{"status":"ok","timestamp":1756217198964,"user_tz":-330,"elapsed":152,"user":{"displayName":"akshay chauhan","userId":"17758583881823287951"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["import google.generativeai as genai\n","from google.colab import userdata\n","\n","GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n","genai.configure(api_key=GOOGLE_API_KEY)"],"metadata":{"id":"dCuZKyRe6GTA","executionInfo":{"status":"ok","timestamp":1756217200784,"user_tz":-330,"elapsed":1050,"user":{"displayName":"akshay chauhan","userId":"17758583881823287951"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["# project variables\n","\n","myproject = \"conversational-bi-080825\"\n","mylocation = \"us-central1\"\n","myapi_endpoint = \"us-central1-aiplatform.googleapis.com\"\n"],"metadata":{"id":"vZXXHv6wtp1k","executionInfo":{"status":"ok","timestamp":1756217200847,"user_tz":-330,"elapsed":56,"user":{"displayName":"akshay chauhan","userId":"17758583881823287951"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["from google import genai\n","\n","class CustomerVertexAILLM(BaseLLM):\n","    \"\"\"Custom LangChain LLM wrapper for Vertex AI Gemini models.\"\"\"\n","\n","    # Default generation settings\n","    generation_config: Dict = {\n","        \"temperature\": 0,\n","        \"top_p\": 0.95,\n","        \"seed\": 0,\n","        \"max_output_tokens\": 8192\n","    }\n","\n","    # Vertex AI content moderation configuration\n","    safety_settings: List[SafetySetting] = [\n","        SafetySetting(category=\"HARM_CATEGORY_HATE_SPEECH\", threshold=\"BLOCK_MEDIUM_AND_ABOVE\"),\n","        SafetySetting(category=\"HARM_CATEGORY_DANGEROUS_CONTENT\", threshold=\"BLOCK_MEDIUM_AND_ABOVE\"),\n","        SafetySetting(category=\"HARM_CATEGORY_SEXUALLY_EXPLICIT\", threshold=\"BLOCK_MEDIUM_AND_ABOVE\"),\n","        SafetySetting(category=\"HARM_CATEGORY_HARASSMENT\", threshold=\"BLOCK_MEDIUM_AND_ABOVE\"),\n","    ]\n","\n","    def _generate(\n","        self,\n","        prompts: List[str],\n","        stop: Optional[List[str]] = None,\n","        **kwargs: Any\n","    ) -> LLMResult:\n","\n","        # ===== 1. Initialize Vertex AI =====\n","        vertexai.init(\n","            project=myproject,        # replace with your project ID\n","            location=mylocation,      # replace with your region, e.g., \"us-central1\"\n","            api_endpoint=myapi_endpoint  # optional, usually auto-detected\n","        )\n","\n","        # ===== 2. Create genai client =====\n","        llm_client = genai.Client(\n","            vertexai=True,\n","            project=myproject,\n","            location=mylocation\n","        )\n","\n","        model_name = \"gemini-2.5-flash\"\n","        generations: List[List[Generation]] = []\n","\n","        # ===== 3. Loop through each prompt =====\n","        for prompt in prompts:\n","            # Convert prompt to Vertex AI content format\n","            contents = [\n","                types.Content(\n","                    role=\"user\",\n","                    parts=[types.Part.from_text(text=prompt)]\n","                )\n","            ]\n","\n","            # ===== 4. Build generation configuration =====\n","            generate_content_config = types.GenerateContentConfig(\n","                temperature=self.generation_config.get(\"temperature\", 0),\n","                top_p=self.generation_config.get(\"top_p\", 0.95),\n","                seed=self.generation_config.get(\"seed\", 0),\n","                max_output_tokens=self.generation_config.get(\"max_output_tokens\", 8192),\n","                safety_settings=self.safety_settings,\n","                thinking_config=types.ThinkingConfig(\n","                    thinking_budget=-1  # unlimited reasoning budget\n","                ),\n","            )\n","\n","            # ===== 5. Call Gemini model =====\n","            try:\n","                response = llm_client.models.generate_content(\n","                    model=model_name,\n","                    contents=contents,\n","                    config=generate_content_config,\n","                )\n","\n","                # Extract text safely\n","                text = \"\"\n","                if response.candidates and response.candidates[0].content.parts:\n","                    text = response.candidates[0].content.parts[0].text\n","\n","            except Exception as e:\n","                text = f\"Error during model generation: {e}\"\n","                print(text)  # Debugging\n","\n","            # Append generation result\n","            generations.append([Generation(text=text)])\n","\n","        # ===== 6. Return LangChain-standard result =====\n","        return LLMResult(generations=generations)\n","\n","    @property\n","    def _llm_type(self) -> str:\n","        return \"custom_vertexai\"\n"],"metadata":{"id":"6efnXLcitwug","executionInfo":{"status":"ok","timestamp":1756217201723,"user_tz":-330,"elapsed":14,"user":{"displayName":"akshay chauhan","userId":"17758583881823287951"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["\n","\n","class CustomVertexAIEmbeddings(Embeddings):\n","    model_name: str = \"text-embedding-004\"\n","    batch_size: int = 200\n","\n","    def __init__(self,\n","                 model_name: str = \"text-embedding-004\",\n","                 project: str = myproject,\n","                 location: str = mylocation,\n","                 api_endpoint: str = myapi_endpoint):\n","        super().__init__()\n","        vertexai.init(project=project, location=location, api_endpoint=api_endpoint)\n","        self.model = TextEmbeddingModel.from_pretrained(model_name)\n","\n","    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n","        all_embeddings = []\n","        try:\n","            for i in range(0, len(texts), self.batch_size):\n","                batch = texts[i:i + self.batch_size]\n","                embeddings = self.model.get_embeddings(batch)\n","                all_embeddings.extend([embedding.values for embedding in embeddings])\n","            return all_embeddings\n","        except Exception as e:\n","            print(f\"Error embedding documents: {e}\")\n","            return [[] for _ in texts]\n","\n","    def embed_query(self, text: str) -> List[float]:\n","        try:\n","            embedding = self.model.get_embeddings([text])[0]\n","            return embedding.values\n","        except Exception as e:\n","            print(f\"Error embedding query: {e}\")\n","            return []\n","\n","    @property\n","    def _embedding_type(self) -> str:\n","        return \"custom_vertexai_embeddings\"\n"],"metadata":{"id":"1O3p8jKUGCYQ","executionInfo":{"status":"ok","timestamp":1756217202771,"user_tz":-330,"elapsed":13,"user":{"displayName":"akshay chauhan","userId":"17758583881823287951"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["class NL2SQLResponse(BaseModel):\n","  sql_query:str = Field(description = \"The SQL Query\")\n","  explaination:str = Field(description = \"Explaination of how SQL Query was generated\")\n","  assumption:str = Field(description = \"Assumptions made while generating SQL Query\")\n","\n","parser = PydanticOutputParser(pydantic_object= NL2SQLResponse)\n","format_instruction = parser.get_format_instructions()"],"metadata":{"id":"Nxi9hUijHdRn","executionInfo":{"status":"ok","timestamp":1756217204129,"user_tz":-330,"elapsed":7,"user":{"displayName":"akshay chauhan","userId":"17758583881823287951"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["class BusinessAssistantBackend:\n","  def __init__(self):\n","    vertexai.init(\n","        project=myproject,\n","        location=mylocation,\n","        api_endpoint=myapi_endpoint\n","    )\n","    self.credentials, self.project = google.auth.default()\n","    aiplatform.init(project=self.project,credentials=self.credentials)\n","\n","    self.llm = CustomerVertexAILLM()\n","    self.embeddings = CustomVertexAIEmbeddings()\n","  def llm_call(self,query):\n","    return self.llm.generate([query]).generations[0][0].text\n","\n","\n","backend = BusinessAssistantBackend()\n","embeddings = backend.embeddings"],"metadata":{"id":"5h5hsIWfJZ3s","executionInfo":{"status":"ok","timestamp":1756217205358,"user_tz":-330,"elapsed":302,"user":{"displayName":"akshay chauhan","userId":"17758583881823287951"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["def format_conversation_history(history:list)-> str:\n","  conversation = \"\"\n","  for turn in history:\n","    if turn['role'] == 'user':\n","      conversation+=f\"\\nUser: {turn['content']}\"\n","    elif turn['role'] == 'Agent':\n","      conversation+=f\"\\nAssistant: {turn['content']}\"\n","  return conversation.strip()\n","\n","\n","def remove_query_ready_flag(text):\n","  try:\n","    pattern = r'\\[?\\s*query_ready\\s*:\\s*(True|Flase)\\s*]?'\n","    cleaned_text = re.sub(pattern,'',text)\n","    return re.sub(r'\\s{2,}',' ',cleaned_text).strip()\n","  except Exception as e:\n","    return text\n","\n","\n","def normalize_scores(results):\n","  scores = [r[\"score\"] for r in results]\n","  min_s, max_s = min(scores), max(scores)\n","  for r in results:\n","    r[\"norm_score\"] = (r['score']-min_s) / (max_s-min_s + 1e-8)\n","  return results"],"metadata":{"id":"jYP6mF4rKZbB","executionInfo":{"status":"ok","timestamp":1756217206008,"user_tz":-330,"elapsed":9,"user":{"displayName":"akshay chauhan","userId":"17758583881823287951"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["# RAG Architecture functions related to query generation and clarification\n","def query_ner(query):\n","  tagger = SequenceTagger.load(\"ner\")\n","\n","  sentence = Sentence(query.upper())\n","\n","  tagger.predict(sentence)\n","  entities = [entity.text for entity in sentence.get_spans(\"ner\")]\n","\n","  return entities\n","\n","def hybrid_search(query,distinct_value_df,dashboard):\n","  texts = distinct_value_df['Value'].astype(str).tolist()\n","  columns = distinct_value_df['Column_Name'].astype(str).tolist()\n","  corpus = [f'Value: {v}, Column: {c}' for v,c in zip(texts,columns)]\n","\n","  tokenized_corpus = [word_tokenize(doc.lower()) for doc in corpus]\n","  bm25 = BM25Okapi(tokenized_corpus)\n","\n","  if dashboard == \"Dashboard1\":\n","    index = faiss.read_index('/content/drive/MyDrive/Colab Notebooks/Conversational BI/RAG/db1_index.faiss')\n","    with open('/content/drive/MyDrive/Colab Notebooks/Conversational BI/RAG/db1_metadata.pkl','rb') as f:\n","      docs = pickle.load(f)\n","\n","  elif dashboard == \"Call_Center_Interaction\":\n","    index = faiss.read_index('/content/drive/MyDrive/Colab Notebooks/Conversational BI/RAG/Call_center_interaction_index.faiss')\n","    with open('/content/drive/MyDrive/Colab Notebooks/Conversational BI/RAG/db1_metadata.pkl','rb') as f:\n","      docs = pickle.load(f)\n","\n","\n","\n","  # Faiss results\n","  query_emebedding = embeddings.embed_query(query)\n","  query_vec = normalize(np.array([query_emebedding],dtype='float32'),axis=1)\n","  D_faiss,I_faiss = index.search(query_vec,6)\n","  faiss_results = [{'Value':corpus[i],\"score\":float(D_faiss[0][j]),'source':'faiss'} for j,i in enumerate(I_faiss[0])]\n","\n","  # BM25 results\n","\n","  query_tokens = word_tokenize(query.lower())\n","  bm25_scores = bm25.get_scores(query_tokens)\n","  top_bm25 = sorted(enumerate(bm25_scores),key=lambda x: x[1], reverse=True)[:6]\n","  bm25_results = [{'Value':corpus[i],\"score\":float(s),'source':'bm25'} for i,s in top_bm25]\n","\n","\n","  faiss_results = normalize_scores(faiss_results)\n","  bm25_results = normalize_scores(bm25_results)\n","\n","  merged = defaultdict(lambda: {'bm25':0,'faiss':0})\n","\n","  for r in faiss_results+bm25_results:\n","    merged[r['Value']][r['source']] = r['norm_score']\n","\n","\n","  final = [{\"Value\":val,\"score\":0.5*v['bm25']+0.5*v['faiss']} for val,v in merged.items()]\n","  final = sorted(final,key = lambda x:x['score'],reverse=True)[:8]\n","\n","  # Reranking with cross encoder\n","\n","  cross_encoder = CrossEncoder('BAAI/bge-reranker-large')\n","  pairs = [(query,r['Value']) for r in final]\n","  rerank_scores = cross_encoder.predict(pairs)\n","\n","  reranked_scores = cross_encoder.predict(pairs)\n","\n","  reranked = sorted(zip(final,rerank_scores),key = lambda x:x[1], reverse = False)\n","  final_res = \"\"\n","  for r,score in reranked:\n","    final_res+=f'{r[\"Value\"]} ' +\"\\n\"\n","  return final_res\n","\n","\n","# Dense RAG functions - FAISS\n","def dense_rag_retreival(dashboard,query):\n","  if dashboard == 'Dashboard1':\n","    index = faiss.read_index('/content/drive/MyDrive/Colab Notebooks/Conversational BI/RAG/db1_index.faiss')\n","    with open('/content/drive/MyDrive/Colab Notebooks/Conversational BI/RAG/db1_metadata.pkl','rb') as f:\n","      docs = pickle.load(f)\n","\n","  elif dashboard == 'Call_Center_Interaction':\n","    index = faiss.read_index('/content/drive/MyDrive/Colab Notebooks/Conversational BI/RAG/Call_center_interaction_index.faiss')\n","    with open('/content/drive/MyDrive/Colab Notebooks/Conversational BI/RAG/db1_metadata.pkl','rb') as f:\n","      docs = pickle.load(f)\n","\n","\n","  else:\n","    return \"  \"\n","\n","  query_embedding = embeddings.embed_query(query)\n","  query_vec = normalize(np.array([query_embedding],dtype='float32'),axis=1)\n","\n","  k = 15\n","  D,I = index.search(query_vec,k)\n","  threshold = 0.60\n","\n","  samples = []\n","  for idx,score in zip(I[0],D[0]):\n","    if idx < len(docs):\n","      sample = {\"Column_Name\": str(docs[idx]['metadata']),\"Value\":str(docs[idx]['text']),\"Score\":str(score)}\n","      samples.append(sample)\n","  sample_string = json.dumps(samples)\n","  return sample_string\n"],"metadata":{"id":"Qg4S2joPNFBO","executionInfo":{"status":"ok","timestamp":1756217206806,"user_tz":-330,"elapsed":23,"user":{"displayName":"akshay chauhan","userId":"17758583881823287951"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["# Clarification prompt info provider function\n","\n","def clari_prompt_info_provider(dashboard):\n","  if dashboard == 'Dashboard1':\n","    db1_dd = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Conversational BI/Dictionary/DB1_dd - Sheet1.csv').to_markdown()\n","    clari_prompt_db1 = db1Clarification\n","    payload = {\"dd_db1\":db1_dd,\"prompt\":clari_prompt_db1}\n","    return payload\n","\n","  elif dashboard == 'Call_Center_Interaction':\n","    db1_dd = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Conversational BI/Dictionary/dd_call_center_interaction.csv').to_markdown()\n","    clari_prompt_db1 = db1Clarification\n","    payload = {\"dd_db1\":db1_dd,\"prompt\":clari_prompt_db1}\n","    return payload\n","\n","# clarification prompt builder\n","def clarification_prompt_builder(dashboard,history):\n","  if len(history) == 1:\n","    query = str(history[-1][\"content\"])\n","  else:\n","    query = str(history[0][\"content\"]) + \" \" + str(history[-1][\"content\"])\n","\n","  if dashboard == 'Dashboard1':\n","    payload = clari_prompt_info_provider(dashboard)\n","    dense_rag_data = dense_rag_retreival(dashboard,query)\n","    total_rag_data = dense_rag_data + \"\\n\" #+ ner_hybrid_rag_data\n","\n","    prompt = PromptTemplate(\n","        input_variables = [\"table_nm\",\"schema\",\"rag_data\",\"history\"],\n","        template = payload['prompt'].strip()\n","    )\n","\n","    formatted_prompt = prompt.format(\n","        table_nm = 'table_dashboard1',\n","        schema = payload['dd_db1'],\n","        rag_data = total_rag_data,\n","        history = format_conversation_history(history)\n","    )\n","\n","    return formatted_prompt\n","\n","  elif dashboard == 'Call_Center_Interaction':\n","    payload = clari_prompt_info_provider(dashboard)\n","    dense_rag_data = dense_rag_retreival(dashboard,query)\n","    total_rag_data = dense_rag_data + \"\\n\" #+ ner_hybrid_rag_data\n","\n","    prompt = PromptTemplate(\n","        input_variables = [\"table_nm\",\"schema\",\"rag_data\",\"history\"],\n","        template = payload['prompt'].strip()\n","    )\n","\n","    formatted_prompt = prompt.format(\n","        table_nm = 'table_call_center_interaction',\n","        schema = payload['dd_db1'],\n","        rag_data = total_rag_data,\n","        history = format_conversation_history(history)\n","    )\n","\n","    return formatted_prompt\n","\n","  else:\n","    print(\"no prompt\", dashboard)"],"metadata":{"id":"F0Le0oZzDBUH","executionInfo":{"status":"ok","timestamp":1756217207877,"user_tz":-330,"elapsed":17,"user":{"displayName":"akshay chauhan","userId":"17758583881823287951"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["# SQL Generation prompt info provider\n","\n","def sql_gen_prompt_info_provider(dashboard):\n","  if dashboard == \"Dashboard1\":\n","    db1_dd = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Conversational BI/Dictionary/DB1_dd - Sheet1.csv').to_markdown()\n","    sql_gen_prompt = db1Main\n","    payload = {'dd_db1':db1_dd,\"prompt\":sql_gen_prompt}\n","    return payload\n","\n","  elif dashboard == \"Call_Center_Interaction\":\n","    db1_dd = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Conversational BI/Dictionary/dd_call_center_interaction.csv').to_markdown()\n","    sql_gen_prompt = db1Main\n","    payload = {'dd_db1':db1_dd,\"prompt\":sql_gen_prompt}\n","    return payload\n","\n","\n","\n","# SQL Generation prompt builder function\n","\n","def sql_gen_prompt_builder(dashboard,plan,chat_history):\n","  if dashboard == \"Dashboard1\":\n","    payload = sql_gen_prompt_info_provider(dashboard)\n","    prompt = PromptTemplate(\n","        input_varibles = [\"table_nm\",\"schema\",\"plan\",\"format_instruction\",\"history\"],\n","        template = payload['prompt'].strip()\n","    )\n","\n","    formatted_prompt = prompt.format(\n","        table_nm = 'table_dashboard1',\n","        schema = payload['dd_db1'],\n","        plan = plan,\n","        format_instruction = format_instruction,\n","        history = format_conversation_history(chat_history)\n","    )\n","    return formatted_prompt\n","\n","  elif dashboard == \"Call_Center_Interaction\":\n","    payload = sql_gen_prompt_info_provider(dashboard)\n","    prompt = PromptTemplate(\n","        input_varibles = [\"table_nm\",\"schema\",\"plan\",\"format_instruction\",\"history\"],\n","        template = payload['prompt'].strip()\n","    )\n","\n","    formatted_prompt = prompt.format(\n","        table_nm = 'table_call_center_interaction',\n","        schema = payload['dd_db1'],\n","        plan = plan,\n","        format_instruction = format_instruction,\n","        history = format_conversation_history(chat_history)\n","    )\n","\n","    return formatted_prompt\n","\n","\n"],"metadata":{"id":"mWcqBMEBE53Q","executionInfo":{"status":"ok","timestamp":1756217208887,"user_tz":-330,"elapsed":15,"user":{"displayName":"akshay chauhan","userId":"17758583881823287951"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["# SQL Summary prompt info provider\n","\n","def sql_summary_prompt_info_provider():\n","  sql_summary_prompt = summarizerPrompt\n","  payload = {\"prompt\":sql_summary_prompt}\n","  return payload\n","\n","\n","# SQL Summary prompt builder function\n","\n","def sql_summary_prompt_builder(question,plan,data):\n","  payload = sql_summary_prompt_info_provider()\n","  prompt = PromptTemplate(\n","      input_varibles = [\"question\",\"plan\",\"data\"],\n","      template = payload['prompt'].strip()\n","  )\n","\n","  formatted_prompt = prompt.format(\n","      question = question,\n","      plan = plan,\n","      data = data\n","  )\n","  return formatted_prompt\n","\n","\n"],"metadata":{"id":"yCSPdJ5ovveC","executionInfo":{"status":"ok","timestamp":1756217210206,"user_tz":-330,"elapsed":4,"user":{"displayName":"akshay chauhan","userId":"17758583881823287951"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["def extract_sql_query(text: str) -> str:\n","    \"\"\"\n","    Extracts SQL query by removing ``` and sql keyword\n","    \"\"\"\n","    return (\n","        text.strip()\n","        .removeprefix(\"```sql\")\n","        .removeprefix(\"```\")\n","        .removesuffix(\"```\")\n","        .strip()\n","    )\n","\n","\n","def execute_sql(query):\n","  sql_data = pd.read_sql(query, conn).to_markdown()\n","  return sql_data"],"metadata":{"id":"T-3ONvGsr1_I","executionInfo":{"status":"ok","timestamp":1756217211153,"user_tz":-330,"elapsed":4,"user":{"displayName":"akshay chauhan","userId":"17758583881823287951"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import sqlite3\n","def create_sql_table(dashboard):\n","  if dashboard == 'Dashboard1':\n","    # 1. Load CSV into Pandas\n","    df = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/Conversational BI/Data/Dashboard1 - Sheet1.csv\")  # update path\n","\n","    # 2. Create SQLite DB (in-memory or a file)\n","    conn = sqlite3.connect(\":memory:\")   # \":memory:\" keeps it in RAM\n","    # conn = sqlite3.connect(\"mydb.sqlite\")  # persistent file-based DB\n","\n","    # 3. Write DataFrame into a SQL table\n","    df.to_sql(\"table_dashboard1\", conn, index=False, if_exists=\"replace\")\n","\n","    # 4. Run SQL queries\n","    # query = \"SELECT column1, COUNT(*) as cnt FROM mytable GROUP BY column1\"\n","    # result = pd.read_sql(query, conn)\n","\n","    # print(result)\n","  elif dashboard == 'Call_Center_Interaction':\n","  # 1. Load CSV into Pandas\n","    df = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/Conversational BI/Data/InteractionSummary_Consolidated (1).xlsx - in.csv\")  # update path\n","\n","    # 2. Create SQLite DB (in-memory or a file)\n","    conn = sqlite3.connect(\":memory:\")   # \":memory:\" keeps it in RAM\n","    # conn = sqlite3.connect(\"mydb.sqlite\")  # persistent file-based DB\n","\n","    # 3. Write DataFrame into a SQL table\n","    df.to_sql(\"table_call_center_interaction\", conn, index=False, if_exists=\"replace\")\n","\n","    # 4. Run SQL queries\n","    # query = \"SELECT column1, COUNT(*) as cnt FROM mytable GROUP BY column1\"\n","    # result = pd.read_sql(query, conn)\n","\n","    # print(result)\n","  return df, conn"],"metadata":{"id":"I5iX0tNHc3dI","executionInfo":{"status":"ok","timestamp":1756217212157,"user_tz":-330,"elapsed":6,"user":{"displayName":"akshay chauhan","userId":"17758583881823287951"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["if __name__ == \"__main__\":\n","\n","  chat_history = []\n","  current_dashboard = 'Call_Center_Interaction'\n","  previous_dashboard = None\n","  df, conn = create_sql_table(current_dashboard)\n","\n","  print(\"Business Assistant is ready. Type 'exit' or 'q' to quit\")\n","\n","  while True:\n","    user_input = input(\"You: \").strip()\n","    if user_input.lower() in ['exit','q']:\n","      print('GoodBye!')\n","      break\n","\n","    chat_history.append({'role':'user','content': user_input})\n","\n","    print(f\"Dashboard Context: {current_dashboard}\")\n","    start_time1 = time()\n","    if previous_dashboard != current_dashboard:\n","      print(f\"Initializing context for '{current_dashboard}'. Resetting conversation history for this topic\")\n","      chat_history = [msg for msg in chat_history if msg['role'] == 'user'][-1:]\n","      previous_dashboard = current_dashboard\n","    print('Final Chat History for LLM:', chat_history)\n","    prompt_clarification = clarification_prompt_builder(current_dashboard,chat_history)\n","    response_plan = backend.llm_call(prompt_clarification)\n","    end_time1 = time()\n","    print(response_plan)\n","    print(\"time elapsed in clarification: \",end_time1 - start_time1,\"s\")\n","\n","    if \"True\".lower() in response_plan.lower():\n","      start_time2 = time()\n","      reponse_temp = remove_query_ready_flag(response_plan)\n","      sql_gen_prompt = sql_gen_prompt_builder(current_dashboard, reponse_temp,chat_history)\n","      response_query = backend.llm_call(sql_gen_prompt)\n","      parsed_output = parser.parse(response_query)\n","      sql_query = parsed_output.sql_query\n","      # new additions\n","      extracted_sql_query = extract_sql_query(sql_query)\n","      sql_query_res = execute_sql(extracted_sql_query)\n","      print(f\"Agent: Generated SQL: \\n{sql_query}\")\n","      end_time2 = time()\n","      print(\"time elapsed in main: \",end_time2 - start_time2,\"s\")\n","      # new additions\n","      sql_summary_prompt = sql_summary_prompt_builder(user_input,reponse_temp,sql_query_res)\n","      start_time3 = time()\n","      response_summary = backend.llm_call(sql_summary_prompt)\n","      print(response_summary)\n","      end_time3 = time()\n","      print(\"time elapsed in summary: \",end_time3 - start_time3,\"s\")\n","      chat_history.append({'role': 'Agent', \"content\": sql_query})\n","      if len(chat_history) > 10:\n","        chat_history = chat_history[-10:]\n","      chat_history[1:-1] = []\n","    else:\n","      chat_history.append({\"role\": \"Agent\", \"content\": response_plan})\n","      if len(chat_history) > 10:\n","        chat_history = chat_history[-10:]\n","\n","    print(\"Total time\",time()-start_time1,'s')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CVDfu2YmZ09s","executionInfo":{"status":"ok","timestamp":1756217262514,"user_tz":-330,"elapsed":48298,"user":{"displayName":"akshay chauhan","userId":"17758583881823287951"}},"outputId":"eaf09d04-d3cf-44a6-99cd-af0d2faecf50"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["Business Assistant is ready. Type 'exit' or 'q' to quit\n","You: county wise conversion rate\n","Dashboard Context: Call_Center_Interaction\n","Initializing context for 'Call_Center_Interaction'. Resetting conversation history for this topic\n","Final Chat History for LLM: [{'role': 'user', 'content': 'county wise conversion rate'}]\n","```sql\n","SELECT\n","  County,\n","  CAST(SUM(Enrollments) AS FLOAT64) / NULLIF(COUNT(ContactId), 0) AS conversion_rate\n","FROM table_call_center_interaction\n","GROUP BY\n","  County;\n","```\n","[query_ready: True]\n","time elapsed in clarification:  5.935588836669922 s\n","Agent: Generated SQL: \n","```sql\n","SELECT\n","  County,\n","  ROUND(CAST(SUM(Enrollments) AS FLOAT64) / NULLIF(COUNT(ContactId), 0), 2) AS conversion_rate\n","FROM\n","  table_call_center_interaction\n","GROUP BY\n","  County;\n","```\n","time elapsed in main:  4.738275766372681 s\n","**User Intent**\n","   - The user's intent was to determine the conversion rate (enrollments per contact) for each county.\n","\n","**Result Summary**\n","   - The query successfully calculated the conversion rate for 15 distinct counties, including one unnamed county. The conversion rates exhibit a wide range, from 0% to 75%.\n","\n","**Insights & Analysis**\n","   - **Wide Variance**: Conversion rates vary significantly across counties, from a low of 0% (Middlesex) to a high of 75% (unnamed county).\n","   - **Top Performers**: The unnamed county (75%), Napa (70%), and Mahoning (65%) demonstrate exceptionally high conversion rates, significantly outperforming most other counties.\n","   - **Underperformers**: Middlesex county has a 0% conversion rate, indicating no enrollments from contacts. Ohio county also shows a very low rate at 5%.\n","   - **Common Performance Cluster**: Several counties (Bourbon, Henry, Lauderdale, Whiteside) cluster around a 25% conversion rate, suggesting a common baseline performance for a segment of the counties.\n","   - **Mid-Range Performance**: Counties like Clark (30%), Dekalb (38%), Harris (44%), York (45%), and New York (58%) fall into a moderate performance bracket.\n","   - **Data Anomaly**: The presence of an unnamed county with the highest conversion rate is an anomaly that requires further investigation to identify the county and understand its success factors.\n","\n","**Business Takeaways**\n","   - **Replicate Success**: Analyze the strategies, demographics, or operational practices in the top-performing counties (unnamed, Napa, Mahoning) to identify best practices that can be replicated across other regions to boost overall conversion rates.\n","   - **Address Performance Gaps**: Prioritize investigation into Middlesex and Ohio counties to understand the root causes of their extremely low conversion rates (0% and 5%). This could involve examining lead quality, agent training, local market challenges, or data accuracy.\n","   - **Targeted Improvement Initiatives**: Develop specific action plans for counties in the mid-range (e.g., 25-45%) to incrementally improve their conversion rates, potentially through targeted training, revised scripts, or localized marketing efforts.\n","   - **Data Governance**: Address the \"unnamed county\" data entry issue to ensure all future data is properly attributed, improving data quality and analytical precision.\n","time elapsed in summary:  7.696129083633423 s\n","Total time 18.370561122894287 s\n","You: q\n","GoodBye!\n"]}]},{"cell_type":"code","source":["# Run SQL queries\n","query = \"\"\"\n","SELECT\n","    *\n","FROM\n","    table_call_center_interaction\n","    limit 1;\n","    \"\"\"\n","result = pd.read_sql(query, conn)\n","\n","print(result)"],"metadata":{"id":"da4_geRoDwOa","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1756198159058,"user_tz":-330,"elapsed":68,"user":{"displayName":"akshay chauhan","userId":"17758583881823287951"}},"outputId":"647c6f62-23d1-4da6-8c19-15c6634915b1"},"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["  State        AgentName CallStatus  CallType        CampaignPocName  \\\n","0    AL  Robert Thompson   Answered  Callback  HealthPlan United-IVR   \n","\n","    Channel County                Factor1         Manager            Partner  \\\n","0  CarrierX   None  HealthPlan United-IVR  Jessica Martel  HealthPlan United   \n","\n","   ...      Newani OriginalLeadCost  RecordId StatTime SubDisposition  \\\n","0  ...  56788999.0              0.0  386842.0     None           None   \n","\n","   TeamCaptain TotalCost  UniqueCalls   ZIP   MA  \n","0     John Doe       $0        0.2283  None  0.0  \n","\n","[1 rows x 50 columns]\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"PIHf7S8erCz6"},"execution_count":null,"outputs":[]}]}